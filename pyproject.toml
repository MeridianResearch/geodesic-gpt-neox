[project]
name = "gpt-neox"
version = "2.0.0"
description = "An open-source library for training large-scale language models on GPUs"
requires-python = ">=3.10"
license = "Apache-2.0"
authors = [
    { name = "EleutherAI", email = "contact@eleuther.ai" }
]

dependencies = [
    # Core ML dependencies
    # Pin to 2.5.x to match conda neox env - compatible with triton 3.5.1 and TE 1.12.0
    "torch>=2.5.0,<2.6.0",
    # NVIDIA libraries needed for transformer-engine (not bundled with PyTorch 2.6.x)
    "nvidia-cudnn-cu12>=9.0.0",
    "nvidia-cublas-cu12>=12.0.0",
    "nvidia-nccl-cu12>=2.20.0",
    "datasets>=4.0.0",
    "transformers>=4.50.0",
    "huggingface-hub>=0.30.0",
    "accelerate>=1.0.0",

    # Training infrastructure
    "deepspeed",  # from git via [tool.uv.sources]
    "wandb>=0.10.28",
    "einops>=0.8.0",
    "pytorch-triton",  # From PyTorch index - has aarch64 wheels

    # Evaluation
    "lm-eval>=0.4.0,<=0.4.1",

    # Tokenization
    "ftfy>=6.0.1",
    "tiktoken>=0.1.2",
    "tokenizers>=0.12.1",
    "sentencepiece",

    # Data loading
    "lm-dataformat",  # from git via [tool.uv.sources]

    # Utilities
    "jinja2>=3.1.4",
    "numpy>=2.0.0",
    "pybind11>=2.6.2",
    "regex",
    "six",
    "pyyaml>=6.0",
    "mpi4py>=3.0.3",
    "toml>=0.10.2",
]

[project.optional-dependencies]
dev = [
    "autopep8>=1.5.6",
    "clang-format>=13.0.1",
    "packaging>=23.0",
    "pre-commit>=2.17.0",
    "pytest>=8.0.0",
    "pytest-cov>=2.11.1",
    "pytest-forked>=1.3.0",
    "pytest-html>=4.1.1",
    "pytest-xdist",
]
flash-attn = [
    "flash-attn>=2.6.3",
]
transformer-engine = [
    "transformer-engine[pytorch]>=1.12.0",
]
full = [
    "flash-attn>=2.6.3",
    "transformer-engine[pytorch]>=1.12.0",
]

[project.urls]
Homepage = "https://www.github.com/eleutherai/gpt-neox"
Repository = "https://www.github.com/eleutherai/gpt-neox"
Documentation = "https://www.github.com/eleutherai/gpt-neox"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["megatron"]

# =============================================================================
# UV Configuration
# =============================================================================

[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv.sources]
# PyTorch packages from cu124 index (for ARM + CUDA support)
# Note: PyTorch 2.5.1 with aarch64 only available for cu124
torch = { index = "pytorch-cu124" }
torchvision = { index = "pytorch-cu124" }
torchaudio = { index = "pytorch-cu124" }
pytorch-triton = { index = "pytorch-cu124" }
# Git dependencies
deepspeed = { git = "https://github.com/EleutherAI/DeeperSpeed.git", rev = "91d1c55ba037b5ada99ae14884dff87a4dc5b9ea" }
lm-dataformat = { git = "https://github.com/EleutherAI/lm_dataformat.git", rev = "4eec05349977071bf67fc072290b95e31c8dd836" }

# =============================================================================
# Tool Configuration
# =============================================================================

[tool.pytest.ini_options]
testpaths = ["tests"]
markers = [
    "cpu: marks tests as CPU-only (deselect with '-m \"not cpu\"')",
]

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "W", "I"]
ignore = ["E501"]  # Line too long - handled by formatter
